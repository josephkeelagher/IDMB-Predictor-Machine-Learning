{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['num_critic_for_reviews', 'duration', 'director_facebook_likes',\n",
      "       'gross', 'num_voted_users', 'num_user_for_reviews', 'country',\n",
      "       'title_year', 'movie_facebook_likes', 'average_degree_centrality',\n",
      "       'doc2vec_genres_1', 'doc2vec_genres_2'],\n",
      "      dtype='object')\n",
      "   num_critic_for_reviews  duration  director_facebook_likes      gross  \\\n",
      "0                     186        73                       28  422783777   \n",
      "1                     252        97                        0   20433940   \n",
      "2                     232       117                      234     371897   \n",
      "3                     297       109                        0   13782838   \n",
      "4                     297       171                        0  313837577   \n",
      "\n",
      "   num_voted_users  num_user_for_reviews  country  title_year  \\\n",
      "0           644348                   656        3        1994   \n",
      "1            78883                   662        0        2005   \n",
      "2            36494                   118        3        2013   \n",
      "3           258078                   911        3        1982   \n",
      "4          1238746                  5060        0        2001   \n",
      "\n",
      "   movie_facebook_likes  average_degree_centrality  doc2vec_genres_1  \\\n",
      "0                 17000                   0.001576          0.000850   \n",
      "1                     0                   0.000675         -0.001613   \n",
      "2                 11000                   0.003002          0.000536   \n",
      "3                 23000                   0.001726          0.000681   \n",
      "4                 21000                   0.001876         -0.001218   \n",
      "\n",
      "   doc2vec_genres_2  \n",
      "0         -0.002809  \n",
      "1         -0.000480  \n",
      "2          0.015579  \n",
      "3         -0.005102  \n",
      "4          0.001817  \n",
      "0    4\n",
      "1    2\n",
      "2    2\n",
      "3    4\n",
      "4    4\n",
      "Name: imdb_score_binned, dtype: int64\n",
      "   num_critic_for_reviews  duration  director_facebook_likes      gross  \\\n",
      "0                      27       118                       14    2246000   \n",
      "1                     339       141                        0   47307550   \n",
      "2                      78        95                       89      37606   \n",
      "3                     226       117                        0  104054514   \n",
      "4                      97       104                       38    3447339   \n",
      "\n",
      "   num_voted_users  num_user_for_reviews  country  title_year  \\\n",
      "0             2302                    20        3        2015   \n",
      "1           104301                   269        3        2012   \n",
      "2            31836                    90        0        2009   \n",
      "3           200359                  1009        3        2002   \n",
      "4            29517                    79        3        2013   \n",
      "\n",
      "   movie_facebook_likes  average_degree_centrality  doc2vec_genres_1  \\\n",
      "0                     0                   0.000375          0.001011   \n",
      "1                 28000                   0.002176          0.007910   \n",
      "2                     0                   0.000900         -0.000602   \n",
      "3                     0                   0.003452          0.007924   \n",
      "4                     0                   0.000450         -0.013033   \n",
      "\n",
      "   doc2vec_genres_2  \n",
      "0         -0.002068  \n",
      "1          0.006607  \n",
      "2         -0.002225  \n",
      "3          0.006651  \n",
      "4          0.002626  \n"
     ]
    }
   ],
   "source": [
    "## TEST DATA PRE PROCESS\n",
    "#Plan: Remove categorical, Include genres and country, remove NA, compute MI and redundantness to remove further.\n",
    "# Load in data\n",
    "train_data = pd.read_csv('train_dataset.csv')\n",
    "test_data = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# Load in Doc2Vec genre feature\n",
    "train_D2V_genres = np.load('train_doc2vec_features_genre.npy')\n",
    "test_D2V_genres = np.load('test_doc2vec_features_genre.npy')\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(train_D2V_genres)\n",
    "reduced_train_D2V_genres = pca.transform(train_D2V_genres)\n",
    "reduced_test_D2V_genres = pca.transform(test_D2V_genres)\n",
    "\n",
    "reduced_train_D2V_genres_df = pd.DataFrame(reduced_train_D2V_genres, columns=[f\"doc2vec_genres_{i}\" for i in range(reduced_train_D2V_genres.shape[1])])\n",
    "reduced_test_D2V_genres_df = pd.DataFrame(reduced_test_D2V_genres, columns=[f\"doc2vec_genres_{i}\" for i in range(reduced_test_D2V_genres.shape[1])])\n",
    "\n",
    "# Save id column for later Kaggle submission\n",
    "id_col = test_data['id']\n",
    "train_data = train_data.drop(['id'], axis=1)\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Check which countries have the most high rated appearances\n",
    "# filtered_df = train_data[train_data['imdb_score_binned'] >= 4]\n",
    "# print(filtered_df[['country', 'imdb_score_binned']])\n",
    "\n",
    "# Count the occurrences of each country\n",
    "# country_counts = filtered_df['country'].value_counts()\n",
    "\n",
    "# Replace top 3 rated countries with ordered values \n",
    "def map_country(country):\n",
    "    if country in high_rated_countries:\n",
    "        if country == 'USA':\n",
    "            return 3\n",
    "        elif country == 'UK':\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "high_rated_countries = ['USA', 'UK', 'France']\n",
    "train_data['country'] = train_data['country'].map(map_country)\n",
    "test_data['country'] = test_data['country'].map(map_country)\n",
    "\n",
    "# Drop redundant numeric data\n",
    "#redundant_attributes = ['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', 'facenumber_in_poster', 'average_degree_centrality', 'title_embedding']\n",
    "#train_data = train_data.drop(columns=redundant_attributes, axis=1)\n",
    "#test_data = test_data.drop(columns=redundant_attributes, axis=1)\n",
    "\n",
    "# Split into attributes and labels\n",
    "\n",
    "attributes = train_data.iloc[:, :-1]\n",
    "numeric_attributes = attributes.select_dtypes(include='number')\n",
    "label = train_data.iloc[:, -1]\n",
    "\n",
    "# Concatenate genre D2V to X and test data \n",
    "combined_train = pd.concat([numeric_attributes, reduced_train_D2V_genres_df, label], axis=1)\n",
    "combined_train = combined_train.select_dtypes(include='number')\n",
    "combined_train.dropna(axis=0, inplace=True)\n",
    "test_data = pd.concat([test_data, reduced_test_D2V_genres_df], axis=1)\n",
    "\n",
    "# Split combined data into X and y and have a validation set with 10% of the data\n",
    "X = combined_train.iloc[:2700, :-1]\n",
    "y = combined_train.iloc[:2700, -1]\n",
    "val_X = combined_train.iloc[2700:, :-1]\n",
    "val_y = combined_train.iloc[2700:, -1]\n",
    "\n",
    "# Drop categorical/nominal data\n",
    "\n",
    "corr_matrix = combined_train.corr()\n",
    "corr_with_label = corr_matrix['imdb_score_binned'].sort_values(ascending=False)\n",
    "corr_columns = corr_matrix.columns[abs(corr_matrix['imdb_score_binned']) > 0.1]\n",
    "corr_columns = corr_columns.drop('imdb_score_binned')\n",
    "print(corr_columns)\n",
    "X = X[corr_columns]\n",
    "test_data = test_data[corr_columns]\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [\n",
    "    ('gradient_boosting', GradientBoostingClassifier(n_estimators=200, random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=42)),\n",
    "    ('logistic_regression', LogisticRegression(max_iter=10000)),\n",
    "]\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "chosen_model = StackingClassifier(estimators=base_estimators,final_estimator=RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=42), cv=cv)\n",
    "#chosen_model.fit(X, y)\n",
    "#predictions = chosen_model.predict(test_data)\n",
    "#predictions_df = pd.DataFrame({'id': id_col, 'imdb_score_binned': predictions})\n",
    "#predictions_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE WITH JUST SCALING THE DATA AND THEN USING RANDOM FOREST\n",
    "pipeline = make_pipeline(StandardScaler(), chosen_model)\n",
    "pipeline.fit(X, y)\n",
    "predictions = pipeline.predict(test_data)\n",
    "predictions_df = pd.DataFrame({'id': id_col, 'imdb_score_binned': predictions})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "#kfolds = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42)\n",
    "#cv_scores = cross_val_score(pipeline, X, y, cv=kfolds, scoring='accuracy')\n",
    "\n",
    "#print(\"CV Scores:\", cv_scores)\n",
    "#print(\"Mean CV Score:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores: [0.72712146 0.72712146 0.72212978 0.71214642 0.72166667]\n",
      "Mean Accuracy: 0.7220371602884083\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=42))\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(\"Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
