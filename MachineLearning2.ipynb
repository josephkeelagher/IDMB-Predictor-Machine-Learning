{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['num_critic_for_reviews', 'duration', 'director_facebook_likes',\n",
      "       'gross', 'num_voted_users', 'cast_total_facebook_likes',\n",
      "       'num_user_for_reviews', 'country', 'movie_facebook_likes',\n",
      "       'doc2vec_genres_0', 'doc2vec_genres_1', 'doc2vec_genres_2',\n",
      "       'doc2vec_genres_3', 'doc2vec_genres_4', 'doc2vec_genres_5',\n",
      "       'doc2vec_genres_6', 'doc2vec_genres_7', 'doc2vec_genres_8',\n",
      "       'doc2vec_genres_9'],\n",
      "      dtype='object')\n",
      "   num_critic_for_reviews  duration  director_facebook_likes        gross  \\\n",
      "0                   186.0      73.0                     28.0  422783777.0   \n",
      "1                   252.0      97.0                      0.0   20433940.0   \n",
      "2                   232.0     117.0                    234.0     371897.0   \n",
      "3                   297.0     109.0                      0.0   13782838.0   \n",
      "4                   297.0     171.0                      0.0  313837577.0   \n",
      "\n",
      "   num_voted_users  cast_total_facebook_likes  num_user_for_reviews  country  \\\n",
      "0         644348.0                     6458.0                 656.0      3.0   \n",
      "1          78883.0                     1876.0                 662.0      0.0   \n",
      "2          36494.0                    13607.0                 118.0      3.0   \n",
      "3         258078.0                     1757.0                 911.0      3.0   \n",
      "4        1238746.0                    22342.0                5060.0      0.0   \n",
      "\n",
      "   movie_facebook_likes  doc2vec_genres_0  doc2vec_genres_1  doc2vec_genres_2  \\\n",
      "0               17000.0          0.000306          0.000842         -0.002835   \n",
      "1                   0.0          0.002038         -0.001611         -0.000434   \n",
      "2               11000.0         -0.005443          0.000536          0.015581   \n",
      "3               23000.0          0.002750          0.000693         -0.005088   \n",
      "4               21000.0          0.001620         -0.001207          0.001826   \n",
      "\n",
      "   doc2vec_genres_3  doc2vec_genres_4  doc2vec_genres_5  doc2vec_genres_6  \\\n",
      "0         -0.002494         -0.002799          0.005001          0.001795   \n",
      "1          0.002510          0.000449          0.004026          0.014871   \n",
      "2          0.002084          0.018395         -0.000585          0.000170   \n",
      "3         -0.001394          0.000074         -0.002394         -0.003442   \n",
      "4          0.002566          0.002311         -0.001589         -0.000358   \n",
      "\n",
      "   doc2vec_genres_7  doc2vec_genres_8  doc2vec_genres_9  \n",
      "0         -0.000203         -0.001105         -0.000415  \n",
      "1         -0.009982          0.013535         -0.004653  \n",
      "2          0.001969          0.000839          0.002687  \n",
      "3         -0.005273         -0.001856          0.002796  \n",
      "4         -0.005533          0.000218         -0.001330  \n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_data = pd.read_csv('train_dataset.csv')\n",
    "test_data = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# Load in Doc2Vec genre feature\n",
    "train_D2V_genres = np.load('train_doc2vec_features_genre.npy')\n",
    "test_D2V_genres = np.load('test_doc2vec_features_genre.npy')\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(train_D2V_genres)\n",
    "reduced_train_D2V_genres = pca.transform(train_D2V_genres)\n",
    "reduced_test_D2V_genres = pca.transform(test_D2V_genres)\n",
    "\n",
    "reduced_train_D2V_genres_df = pd.DataFrame(reduced_train_D2V_genres, columns=[f\"doc2vec_genres_{i}\" for i in range(reduced_train_D2V_genres.shape[1])])\n",
    "reduced_test_D2V_genres_df = pd.DataFrame(reduced_test_D2V_genres, columns=[f\"doc2vec_genres_{i}\" for i in range(reduced_test_D2V_genres.shape[1])])\n",
    "\n",
    "# Save id column for later Kaggle submission\n",
    "id_col = test_data['id']\n",
    "train_data = train_data.drop(['id'], axis=1)\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Check which countries have the most high rated appearances\n",
    "# filtered_df = train_data[train_data['imdb_score_binned'] >= 4]\n",
    "# print(filtered_df[['country', 'imdb_score_binned']])\n",
    "\n",
    "# Count the occurrences of each country\n",
    "# country_counts = filtered_df['country'].value_counts()\n",
    "\n",
    "# Replace top 3 rated countries with ordered values \n",
    "def map_country(country):\n",
    "    if country in high_rated_countries:\n",
    "        if country == 'USA':\n",
    "            return 3\n",
    "        elif country == 'UK':\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "high_rated_countries = ['USA', 'UK', 'France']\n",
    "train_data['country'] = train_data['country'].map(map_country)\n",
    "test_data['country'] = test_data['country'].map(map_country)\n",
    "\n",
    "# Drop redundant numeric data\n",
    "redundant_attributes = ['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', 'facenumber_in_poster', 'average_degree_centrality', 'title_embedding']\n",
    "train_data = train_data.drop(columns=redundant_attributes, axis=1)\n",
    "test_data = test_data.drop(columns=redundant_attributes, axis=1)\n",
    "\n",
    "\n",
    "# Remove missing values\n",
    "train_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Split into attributes and labels\n",
    "attributes = train_data.iloc[:, :-1]\n",
    "label = train_data.iloc[:, -1]\n",
    "\n",
    "# Concatenate genre D2V to X and test data \n",
    "combined_train = pd.concat([attributes, reduced_train_D2V_genres_df, label], axis=1)\n",
    "combined_train.dropna(axis=0, inplace=True)\n",
    "test_data = pd.concat([test_data, reduced_test_D2V_genres_df], axis=1)\n",
    "\n",
    "# Split combined data into X and y\n",
    "\n",
    "X = combined_train.iloc[:, :-1]\n",
    "y = combined_train.iloc[:, -1]\n",
    "\n",
    "# Drop categorical/nominal data\n",
    "numeric_attributes = X.select_dtypes(include='number').columns\n",
    "print(numeric_attributes)\n",
    "X = X[numeric_attributes]\n",
    "\n",
    "test_data = test_data[numeric_attributes]\n",
    "\n",
    "print(X.head())\n",
    "\n",
    "#print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [\n",
    "    ('gradient_boosting', GradientBoostingClassifier(n_estimators=200, random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=42)),\n",
    "    ('logistic_regression', LogisticRegression(max_iter=10000)),\n",
    "]\n",
    "chosen_model = StackingClassifier(estimators=base_estimators,final_estimator=RandomForestClassifier(n_estimators=200, criterion='entropy'), cv=5)\n",
    "#chosen_model.fit(X, y)\n",
    "#predictions = chosen_model.predict(test_data)\n",
    "#predictions_df = pd.DataFrame({'id': id_col, 'imdb_score_binned': predictions})\n",
    "#predictions_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE WITH JUST SCALING THE DATA AND THEN USING RANDOM FOREST\n",
    "pipeline = make_pipeline(StandardScaler(), chosen_model)\n",
    "pipeline.fit(X, y)\n",
    "predictions = pipeline.predict(test_data)\n",
    "predictions_df = pd.DataFrame({'id': id_col, 'imdb_score_binned': predictions})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "#kfolds = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42)\n",
    "#cv_scores = cross_val_score(pipeline, X, y, cv=kfolds, scoring='accuracy')\n",
    "\n",
    "#print(\"CV Scores:\", cv_scores)\n",
    "#print(\"Mean CV Score:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores: [0.70549085 0.69717138 0.70715474 0.715      0.69833333]\n",
      "Mean Accuracy: 0.7046300610094287\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=200, criterion='entropy'))\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(\"Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
